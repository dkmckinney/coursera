---
title: 'Human Activity Recognition: Weight Lifting Exercises Dataset Analysis'
author: "dk mckinney"
date: "aug-14"
output:
  html_document:
    highlight: pygments
    theme: united
    toc: yes
  pdf_document:
    highlight: zenburn
    toc: yes
---

## Abstract

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, my goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

## Introduction

This data set attempts to define quality of execution and investigate three aspects that pertain to qualitative activity recognition: the problem of specifying correct execution, the automatic and robust detection of execution mistakes, and how to provide feedback on the quality of execution to the user. This data set utilizes the on-body sensing approach.

Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. Participants were supervised by an experienced weight lifter to make sure the execution complied to the manner they were supposed to simulate. The exercises were performed by six male participants aged between 20-28 years, with little weight lifting experience. We made sure that all participants could easily simulate the mistakes in a safe and controlled manner by using a relatively light dumbbell (1.25kg).

The goal of this analysis is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. This report will describe how I built my model, how I used cross validation, what I think the expected out of sample error is, and why I made the choices I did. I will also use the prediction model to predict 20 different test cases. 

## Necessary Libraries

```{r setup, echo=FALSE}
rm(list=ls())
options(digits=4, width=120)

if (file.exists("c:/windows/win.ini")) {
  setwd("w:/github/coursera") 
  } else setwd("/home/derec/mnt/project/github/coursera") 
```

```{r load_libraries, echo=TRUE}
suppressWarnings(suppressMessages(library(caret)))
suppressWarnings(suppressMessages(library(corrplot)))
suppressWarnings(suppressMessages(library(FactoMineR)))
suppressWarnings(suppressMessages(library(grDevices)))
suppressWarnings(suppressMessages(library(knitr)))
suppressWarnings(suppressMessages(library(randomForest)))
```

## Data Loading and Processing

Data cleansing tasks performed:

* Removal of excel division error strings `#DIV/0!` and replacement with `NA` values.
* Conversion of empty strings to `NA` values.
* Removal of any features that primarily contained NA values.
* Removal of all metadata columns.


```{r load_data}
# download files if they do not exist
if (!file.exists("pml-training.csv")) {
    download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", 
        destfile = "pml-training.csv")
}

if (!file.exists("pml-testing.csv")) {
    download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", 
        destfile = "pml-testing.csv")
}

# read training set and handle na strings
tmp.train <- read.csv("pml-training.csv", na.strings=c("NA","","#DIV/0!"," "), stringsAsFactors=T)
names(tmp.train)[1] <- "id"

# do not keep columns with mostly missing values (80% threshold).
keep <- c((colSums(!is.na(tmp.train[,-ncol(tmp.train)])) >= 0.8*nrow(tmp.train)))
train <- tmp.train[,keep]

# remove unneeded nominal attributes:"user_name","raw_timestamp_part_1","raw_timestamp_part_2","cvtd_timestamp",
# "new_window","num_window" 
train <- train[,-c(2:7)]  
keep <- colnames(train)

# load test set and conform it to match training set.
tmp.test <- read.csv("pml-testing.csv", na.strings=c("NA","","#DIV/0!"," "), stringsAsFactors=T)
names(tmp.test)[1] <- "id"
tmp.test$id <- tmp.test$problem_id # copy problem_id to row number field and use as id
tmp.test <- tmp.test[, -160]
tmp.test$classe <- ""
tmp.test <- tmp.test[,keep]
tmp.test$classe <- factor(tmp.test$classe, levels=c("A","B","C","D","E"))
test <- tmp.test

# dimensions of data sets
dim(train)
dim(test)
```
```{r echo=FALSE}
# remove unneeded variables.
rm(tmp.test,tmp.train)
```

## Feature Selection

The data set for analysis has 52 predictor attributes, so exploratory analysis with scatter plot matrices is impractical. The next step of this analysis will be the attempt to reduce the number of attributes prior to modeling. Instead of allowing the model to select which attributes to use from the processed data set, I will filter the data to remove irrelevant, and highly correlated attributes prior to modeling.


```{r data_reduction}
vis.train <- train[,3:ncol(train)-1]

# scale and center training set without id or classe attributes.
vis.train.scale <- scale(vis.train, center=TRUE, scale=TRUE)
corMat <- cor(vis.train.scale) #compute the correlation matrix
```

Plot 1: Correlation plot of initial set of predictors. Extreme correlation values are colored dark red and dark blue.

```{r plot_1, echo=FALSE, fig.height=10, fig.width=10}
corrplot(corMat, order="FPC", method="color", type="lower", tl.cex=0.75, tl.col=rgb(0,0,0),
         title="")
```

```{r}
# remove highly correlated attributes and re-plot.
highlyCor <- findCorrelation(corMat, 0.80) # set highly correlated at 0.80
vis.Filtered.scale <- vis.train.scale[,-highlyCor]
corMat <- cor(vis.Filtered.scale)
```

Plot 2: Correlation plot of revised set of predictors.

```{r plot_2, echo=FALSE, fig.height=10, fig.width=10}
corrplot(corMat, order="FPC", method="color", type="lower", tl.cex=0.8, tl.col=rgb(0, 0, 0), title="")

vis.keep <- colnames(vis.Filtered.scale)
vis.keep[41] <- "id"
vis.keep[42] <- "classe"

train <- train[,vis.keep]
test <- test[,vis.keep]
```
```{r echo=FALSE}
# remove unneeded variables.
rm(corMat,vis.Filtered.scale,vis.train,vis.train.scale,highlyCor,vis.keep)
```


## Modeling, Testing, and Out-of-sample Error Rate

Having removed the problem attributes, and those that are highly correlated with each other, I will fit a Random Forest clustering model to predict the outcomes of the testing set. With random forests, **there is no need for cross-validation, or a separate test set to get an unbiased estimate of the test set error.** It is estimated internally, during the execution. So, I proceed with training the model on the training data set.
I chose random forests because they have several nice theoretical properties:

* They deal naturally with non-linearity, and assuming linearity in this case would be imprudent.
* There's no parameter selection involved. While random forest may overfit a given data set, just as any other machine learning algorithm, it has been shown by Breiman that classifier variance does not grow with the number of trees used (unlike with Adaboosted decision trees, for example). Therefore, it's always better to use more trees, memory and computational power allowing.
* The algorithm allows for good in-training estimates of variable importance and generalization error[2], which largely eliminates the need for a separate validation stage, though obtaining a proper generalization error estimate on a testing set would still be prudent.
* The algorithm is generally robust to outliers and correlated covariates [2], which seems like a nice property to have when there are known interactions between variables and no data on presence of outliers in the data set.

Given that the problem at hand is a high-dimensional classification problem with number of observations much exceeding the number of predictors, random forest seems like a sound choice.


```{r randomForest, cache=TRUE}
set.seed(123)
mod.train <- train[, -c(41)] # remove "id" attribute
mod.test <- test[, -c(41)] # remove "id" attribute
model <- randomForest(classe ~ ., data=mod.train, ntree=128)
model
```

**I predict my out-of-sample error to be less than 0.5%.** The confusion matrix also looks good, indicating that the model fit the training set well. It may also be instructive to look at the variable importance estimates obtained by the classifier training algorithm.

```{r}
imp <- varImp(model)
imp$Variable <- row.names(imp)
imp[order(imp$Overall, decreasing = T), ]
```
Only three variables have importance measure more than ten times lower than the most important variable (yaw_belt), which seems to indicate the algorithm employed made good use of provided predictors.

The following command can be used to obtain model's prediction for the assigned testing data set.The output has been concealed.

```{r randomForest_predict, cache=TRUE, eval=FALSE}
mod.test$classe <- predict(model, mod.test)
mod.test$classe
```

The predicted values proved to be 100% accurate. The predicted outcomes were identical using 128 trees as they were using 2048 trees.


## Prediction Submittal

This function saves predicted classes to individual files for submittal.

```{r submittal, echo=TRUE}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("submit/problem_id_", i, ".txt")
    write.table(x[i], file=filename, quote=FALSE, row.names=FALSE, col.names=FALSE)
  }
}
pml_write_files(test$classe)
```

## Data for this analysis

Courtesy of:

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013. 

## References

[1] http://www.r-bloggers.com/introduction-to-feature-selection-for-bioinformaticians-using-r-correlation-matrix-filters-pca-backward-selection/

[2] Breiman, L. (2001). Random forests. Machine learning, 45(1), 5-32.